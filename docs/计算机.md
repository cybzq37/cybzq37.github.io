[cs自学指南](https://csdiy.wiki/)
[计算机分类](https://csrankings.org/)
[大模型](https://github.com/liguodongiot/llm-action)
[大模型原理和实战](https://zhuanlan.zhihu.com/p/682907673)
[mlsys](各方向综述)
[CS专业学习路线](https://hackway.org/docs/ai/intro)

https://github.com/awesome-programming-books/awesome-programming-books.github.io
https://github.com/httaotao/glusterfs-book
https://github.com/wangzhiwubigdata/God-Of-BigData
https://github.com/it-ebooks-0/geektime-books
https://github.com/lshmouse/reading-papers
https://github.com/digoal/blog
https://github.com/zouhuigang/book
https://github.com/chungchi300/reading-book/tree/master
https://github.com/zouhuigang/book


人工智能（AI, Artificial Intelligence）
- 计算机视觉 (CV)
- 机器学习 (ML)
	- - 监督学习（分类、回归）
	- 无监督学习（聚类、降维）
	- 强化学习（控制、博弈）
	- 深度学习（Deep Learning）
- 自然语言处理 (NLP)
- 专家系统、机器人等
- 大模型算法
- mlsys
- 网络与信息检索 
- **大模型（Large Language Model, LLM / Foundation Model）**
	深度学习的一个分支成果，属于“应用机器学习”的前沿领域。
	是近年来深度学习发展出的超大规模模型（如 GPT、BERT、LLaMA、Claude），通常训练参数数以百亿、千亿计。
- **MLSys（Machine Learning Systems）
	是**机器学习系统**的研究领域，关注如何让大规模机器学习**更高效、更可扩展、更经济**地运行。
	- 分布式训练（如 PyTorch Distributed、TensorFlow MultiWorker）
	- GPU/TPU 编程
	- 高效推理（ONNX, TensorRT, quantization）
	- 自动化机器学习（AutoML、调参）
	- 数据管道、模型部署、性能优化
	- 系统设计（包括调度、存储、通信）
 
系统
- 计算机架构
- 计算机网络
- 计算机安全
- ~~数据库~~
- 设计自动化
- 嵌入式和实时系统
- 高性能计算
- 移动计算
- 测量和性能分析
- 操作系统
- ~~编程语言 (PL) 需求量极少~~
- 软件工程	
 
 理论
 - 算法与复杂性
 - 密码学
 - 逻辑与验证	
 
 跨学科领域
- 计算机生物学和生物信息学
- 计算机图形
- 计算机科学教育
- 经济学与计算
- 人机交互
- 机器人技术
- 可视化

- AI：人工智能（Artificial intelligence）得益于其“数据驱动”的特点，在近些年得到了广泛的研究与应用。
    
    - Machine learning & data mining：通常指经典的机器学习方法，包括分类（Classification）、回归（Regression）、聚类（Clustering）、降维（Dimensionality Reduction）等子领域，也包括算法的部署（分布式计算、联邦学习等）
    - Computer vision：因互联网中广泛的应用场景而备受重视的研究领域之一，主要任务是提取图像中的特征，将特征应用于分类、目标检测与追踪、分割、检索、补帧、超分辨率等任务；另外与计算机图形学、机器人学、自动驾驶等有紧密的联系
    - Natural language processing：另一备受重视的研究领域，主要处理以文本为主，有翻译、检索等传统研究方向，也有掩码语言模型（Masked Language Model）等新兴研究方向
    - Multimodal learning：“模态”（Modality）指一种信息的表现形式，如图像、文字、语音、动作等。多模态研究重点关注多个模态之间的表示（表示学习）、对齐（Cross-modal Retrival 等）、翻译（Image Captioning 等）、融合（情感分析等）、协同学习（多个模态共享知识）等问题
    - Content Generation：内容生成其实已经包括在之前的 entry 中，这里主要强调使用 AI 进行“创造性”的工作；包括单一模态，如文字续写、图像扩展、图像风格化，也包括多个模态，如从图像生成文字（Image Captioning）、从文字生成图像等
    - Reinforcement learning：不了解，笔者中没有研究该方向的
    - The Web & information retrieval：跟Deep Learning三大应用之一的搜广推密切相关，目前的细分方向主要有认知搜索（与脑机接口技术结合）、神经搜索（以跨模态检索为主，把不同种类的数据按照同样的结构做向量化存储，同时对已有不同数据库支持vector search和filter）以及搜索引擎中的公平性与隐私性（在Trustworthy ML研究范围之内），THUIR的work基本代表了国内最前沿的研究
- System：着重介绍下System系统领域
    
    - Computer Architecture：顾名思义，计算机体系结构主要研究计算机本身的架构等，包括CPU和GPU等的架构，本身这个研究方向会比较偏向硬件；这一部分研究和Operating system的研究会有部分重合；但是更加偏底层一点
    - Computer Networks：计算机网络主要研究为当前的网络环境提出更合适的协议，优化网络协议，软件定义网络，以及对网络架构进行优化和加速等；网络领域的科研往往需要比较大规模的实验，现在的这个方向部分研究开始偏无线一点了（有线做到头了），该领域目前比较关注的热门课题：
        - MLSys：MLSys中的大规模网络优化问题，包括如何降低bubble time等
        - 卫星网络：研究卫星网络如何通信优化、卫星网络的路由算法等（大家可以理解为研究星链类似的东西）
        - Blockchain的网络：研究Blockchain的各种协议如何优化性能等
        - ...
    - Computer security：计算机安全主要研究系统安全、软件安全、网络安全等，现在的安全研究也扩展到了对一些新兴技术的安全研究，比如AI模型本身的安全性等。这里笔者列一些具体的研究方向：
        - ML安全：包括但不限于大语言模型的安全（Jailbreak、Prompt Injection等），FL学习的隐私安全，预训练模型中的数据泄露、数据窃取等
        - 软件漏洞：包括如何挖掘软件漏洞，如何自动化利用软件漏洞等
        - 隐私：包括但不限于用各种差分隐私算法对数据脱敏化，并且也包括各种privacy合规性检查等等（所有跟privacy能扯上边的）
        - ...
    - Databases：数据库当前前沿研究主要聚焦于如何优化数据库的存储速度等
    - Design automation：不了解，笔者中没有研究该方向的
    - Embedded & real time system：不了解，笔者中没有研究该方向的，理论上应该偏向于无线
    - High-performance computing：高性能计算主要是超算相关的技术等，包括一些分布式系统加速，主要就是优化当前的大型大规模计算系统的速度和效率
    - Mobile computing：移动计算相关研究主要关注边缘计算等，比如说边缘计算中的小型AI模型的加速、模型压缩等
    - Measurement & performance analysis：测量等主要关注对当前网络环境的一些测量，比如说测量不同的协议对于用户体验的影响等
    - Operating system：操作系统这个领域主要研究包括操作系统内核优化、内核安全，以及分布式系统优化，包括区块链等研究，特别硬核
    - Programming Languages：编程语言这方面研究属于真·计算机方向的研究，包括程序分析、程序安全和程序合成等，研究的东西很多会涉及到编译原理等，极端硬核
    - Software engineering：软件工程的研究主要包含软件架构、软件分析、软件安全等
- Theory：理论领域主要研究计算机理论，这一部分很多都比较偏数学了
    
    - Algorithms & complexity：你可以理解为你在算法导论里面看到的那些算法等
    - Cryptography：密码学，主要研究加密解密算法那一块东西，现在主要研究同态、零知识证明等算法，基本都是数学，其实这部分有的时候也被归类为计算机安全的相关研究
    - Logic & verification：不了解，笔者中没有研究该方向的
- Interdisciplinary Areas：主要研究计算机与其他领域之间的交叉，一般来说是计算机技术如何应用于其他领域
    
    - Comp. bio & bioinformatics：生物信息学，主要研究如何将计算机技术应用于生物学方面的研究，有人称之为AI for science
    - Computer graphics：计算机图形学，主要研究如何通过计算机模拟现实世界的图像，比如大家知道的Nvidia的一系列技术等，也属于这个范畴
    - Economics & computation：主要研究计算机如何应用于经济学和运筹学等的技术
    - Human-computer interaction：人机交互，这一部分主要是研究如何优化计算机与用户之间的交互，可以分成技术端和人文端，这一个研究领域除了计算机研究人员，还有很多学美术、心理、文学等领域的研究者进行研究
    - Robotics：研究机器人相关技术
    - Visualization：研究AR和VR等相关技术


# ML system 入坑指南

最近ChatGpt大火,越来越多开始关注大模型2,但对于大模型落地而言,除了先进的算法,其背后的MLsystem(机器学习系统), 从分布式训练到高效推理的完整链路同样重要, 好的基础设施是应用爆发的基础.

作为一个入坑MLsys快两年半的练习生, 本文主要围绕自己学习的经历来构筑,会持续更新,希望能给希望入坑的新人一个指引,也给非Mlsys背景但感兴趣的其他领域的同学一些启发.

---

## [](https://fazzie-key.cool/2023/02/21/MLsys/#Course "Course")Course

首先是课程,入坑MLsys,基本的计算机背景知识比如数据结构就不多聊了,更多讲讲一些更加专业性的进阶课程,

### [](https://fazzie-key.cool/2023/02/21/MLsys/#Operating-System "Operating System")Operating System

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#%E5%8D%97%E4%BA%AC%E5%A4%A7%E5%AD%A6JYY-OS "南京大学JYY OS")南京大学JYY OS

南京大学JYY老师开的操作系统课内容非常硬核, workload巨大,课程质量比肩四大

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#MIT-6-S081 "MIT 6.S081")MIT 6.S081

MIT经典OS课,资料,lab都非常全

- [课程主页](https://pdos.csail.mit.edu/6.828/2020/schedule.html)
- [MIT 6.S081 中文 Tutorial Book](https://mit-public-courses-cn-translatio.gitbook.io/mit6-s081/)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#Parallel-computing "Parallel computing")Parallel computing

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#CMU15418-Parallel-computing "CMU15418 Parallel computing")CMU15418 Parallel computing

并行计算非常好的入门课,内容硬核,workload巨大,涉及现代多处理器,CPU加速比如SIMD,分布式通讯协议MPI,GPU加速Cuda编程,异构计算,同步,Cache

- [课程主页](http://15418.courses.cs.cmu.edu/spring2016/)

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#UCB-cs267-Applications-of-Parallel-Computers "UCB cs267 Applications of Parallel Computers")UCB cs267 Applications of Parallel Computers

HPC祖师爷 Jim Demmel 22 spring最新版本,

- [课程主页](https://sites.google.com/lbl.gov/cs267-spr2022?pli=1)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#Distributed-system "Distributed system")Distributed system

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#MIT6-824%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F "MIT6.824分布式系统")MIT6.824分布式系统

这门课推荐的人也非常多了,用go实现,了解传统的分布式系统知识和历史对现代的分布式机器学习系统的学习还是有一定的帮助,不过对于做MLsys不是必须.

- [课程主页](https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#MLSystem "MLSystem")MLSystem

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#CMU-DL-System "CMU DL System")CMU DL System

陈天奇老师的课,涉及nn库实现,自动微分,GPU加速,模型部署和部分AI编译内容,内容除了分布式训练涉及的不够,基础的MLsys还是非常全面的.

- [课程主页](https://dlsyscourse.org/)

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#Mini-torch "Mini torch")Mini torch

完全用python实现的简单torch版本,涉及自动微分,张量,GPU加速.适合新手入门

- [课程主页](https://minitorch.github.io/)

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F-%E8%AE%BE%E8%AE%A1%E5%92%8C%E5%AE%9E%E7%8E%B0 "机器学习系统:设计和实现")机器学习系统:设计和实现

华为Mindspore团队(没错,就是我打过工的Team)和一群大佬AP搞的, 计算图,编译器前后端,分布式训练都有涉及,内容比较全面,比较适合有一定基础的人阅读或者作为工具书.

- [主页](https://openmlsys.github.io/#)

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#System-for-AI "System for AI")System for AI

微软发起的,目前还在快速迭代更新的工具书,舍和补全基础.

- [主页](https://github.com/microsoft/AI-System)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#AI-Compilation "AI Compilation")AI Compilation

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#Machine-Learning-Compilation "Machine Learning Compilation")Machine Learning Compilation

还是陈天奇老师的课,以TVM为基础, AI编译器这样前沿的领域为数不多的课.

- [课程主页](https://mlc.ai/summer22-zh/)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#Large-model "Large model")Large model

对于做ML system 的同学而言,了解一些最新的算法也是非常必要的,不用过度关系一些fancy的技巧,更多关注模型架构,参数,大的范式上的变化即可.  
算法的业界进展确实太快了,很难有系统的课,某些顶级大学会用讲座的形式开展,去讲GPT,PLAM这样的大模型, 看论文和官方网站,blog是更好的选择.

可以看李沐老师论文精讲去follow一些比较新且影响力巨大的工作, Muli is all you need !!!

- [paper-reading github 主页](https://github.com/mli/paper-reading)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#Large-model-training-amp-paper "Large model training & paper")Large model training & paper

这块目前还没有比较系统的课,大规模的分布式训练开始应用也就这几年的事情,也是MLsys领域的最大热点,这里简单总结一下需要掌握的知识点和参考论文

- Data Parallel(数据并行)
- Distributed Data Parallel(分布式数据并行)
    - [PyTorch Distributed: Experiences on Accelerating Data Parallel Training](https://arxiv.org/abs/2006.15704)
- Mix precise Training(混合精度训练)
    - [Mix precise Training](https://arxiv.org/abs/1710.03740)
- Zero Optimizer(零冗余优化器)
    - [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)
- Tensor Parallel(张量并行)
    - 1D并行:[Megatron-LM](https://arxiv.org/abs/1909.08053)
- Pipeline Parallel(流水并行)
    - [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://arxiv.org/abs/1811.06965)
- Auto Parallel(自动并行)
    - [Alpa:Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning](https://arxiv.org/abs/2201.12023)
- Sequence Parallel(序列并行)
    - [Sequence Parallelism: Long Sequence Training from  
        System Perspective](https://arxiv.org/abs/2105.13120)
- Large batchsize(超大batch size)
    - [LARS:Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888)
- MOE(混合专家模型)
    - [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)
- Kernal Fusion(算子融合)
    - [Flash attention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
- Optimizer Fusion(优化器融合)
    - [OPTIMIZER FUSION: EFFICIENT TRAINING WITH BETTER LOCALITY AND PARALLELISM](https://arxiv.org/abs/2104.00237)
- Activation checkpoint
    - [Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/abs/1604.06174)
- Fine-tune accelerate(微调加速)
    - [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- Isomorphic training(异构训练)
    - [PatrickStar: Parallel Training of Pre-trained Models via  
        Chunk-based Dynamic Memory Management](https://arxiv.org/abs/2108.05818)
- Asynchronous Distributed Dataflow(异步数据流)
    - [PATHWAYS: ASYNCHRONOUS DISTRIBUTED DATAFLOW FOR ML](https://arxiv.org/abs/2203.12533)
- Distributed Scheduling(分布式调度)
    - [RLlib: Abstractions for Distributed Reinforcement Learning](https://arxiv.org/abs/1712.09381)
- Quant(量化)
    - [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2211.10438)
- 大语言模型推理
    - [FlexGen: High-throughput Generative Inference of Large Language Models with a Single GPU](https://github.com/FMInference/FlexGen/blob/main/docs/paper.pdf)

> 每个细分领域的论文还有很多,不一一列举了,对于入坑来说,抓住主线即可.

## [](https://fazzie-key.cool/2023/02/21/MLsys/#Programming-Languages "Programming Languages")Programming Languages

### [](https://fazzie-key.cool/2023/02/21/MLsys/#C "C++")C++

- [Cmake](https://www.hahack.com/codes/cmake/)
- [现代CPP](https://changkun.de/modern-cpp/zh-cn/01-intro/)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#Python "Python")Python

Python基础课这个太多了,不作推荐了,做MLsys比较需要掌握用python调用C,比如Cpython,pybind,以及一些python高级特性,比如hook,装饰器

- [pybind](https://github.com/pybind/pybind11)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#Cuda "Cuda")Cuda

这个可以参考的也比较多,英伟达的官方手册永远是最好的参考.

- [Cuda programming guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#OpenCL "OpenCL")OpenCL

对于非Nvidia芯片的设备,比如手机Soc,移动端推理芯片大多不支持cuda,那么用OpenCL来做异构加速就是一个更通用的方案

- [OpenCL异构计算](https://www.bookstack.cn/read/Heterogeneous-Computing-with-OpenCL-2.0/README.md)

# [](https://fazzie-key.cool/2023/02/21/MLsys/#%E5%86%9B%E7%81%AB%E5%BA%93 "军火库")军火库

这里会简单总结我接触或使用和直接参与开发的Mlsys的军火库,会持续更新.

## [](https://fazzie-key.cool/2023/02/21/MLsys/#Framework "Framework")Framework

对于MLsys这样前沿的领域而言,因为很多方面并没有足够的资料,经常被迫去直接学习源码,以实际的case作为学习手段也是非常好的方式.这里简单归类一下我遇到过的MLsys,大多数处于简单了解和使用,有少部分比较深入看过源码.

### [](https://fazzie-key.cool/2023/02/21/MLsys/#Inference "Inference")Inference

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#TensorRT "TensorRT")TensorRT

英伟达的推理方案, 目前整体上在英伟达GPU上做的最好的推理框架,比较是自己的卡.

- [github](https://github.com/NVIDIA/TensorRT)

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#AI-Template "AI Template")AI Template

FaceBook刚搞的一个推理库,在很多硬件上速度性能都超过TensorRT, 还比较新的框架

- [github](https://github.com/facebookincubator/AITemplate)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#Severing "Severing")Severing

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#triton-inference-server "triton-inference-server")triton-inference-server

英伟达的ML serving框架,比较成熟

- [github](https://github.com/triton-inference-server/server)

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#clip-as-service "clip-as-service")clip-as-service

Jina-AI做的,一家中国 start up, 在mass(model as service)上是一个非常不错的落地产品,特别喜欢

- [github](https://github.com/jina-ai/clip-as-service)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#Mobile-inference "Mobile inference")Mobile inference

移动端推理是我比较深入做过的,对其底层了解的比较多

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#Mindsporelite "Mindsporelite")Mindsporelite

我有幸参与写过的推理引擎,对于全流程在mindspore上做的体验还是不错的.

- [gitee](https://gitee.com/mindspore/mindspore)

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#MNN "MNN")MNN

阿里达摩院做的,我写mindsporelite的遇到问题的时候经常被mentor叫去学习一下友商的代码,CPU的一些kernel用汇编写的,这点映像非常深刻,代码质量非常高.

- [github](https://github.com/alibaba/MNN)

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#TensorFlowlite "TensorFlowlite")TensorFlowlite

集成在Tensorflow的移动端推引擎,国际上应该是最早做的移动端推理.没错,TFlite的大哥就是那个从谷歌跑路重回斯坦福读博的皮特·沃登,他写了TinyML这本书,对整个移动端推理都是有重要意义的.我也从这学习了不少.google的代码质量太高了,看注释都能学很多.

- [github](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite)

#### [](https://fazzie-key.cool/2023/02/21/MLsys/#NCNN "NCNN")NCNN

国内做的最早的端侧推理引擎,腾讯搞的,不得不说,很多东西还是需求驱动, 靠各种移动APP为主要产品的中国互联网公司,移动推理引擎做的都不错.

- [github](https://github.com/Tencent/ncnn)

## [](https://fazzie-key.cool/2023/02/21/MLsys/#DeepLearning-Framework "DeepLearning Framework")DeepLearning Framework

大的深度学习的框架的介绍太多了,不一一介绍了,大同小异,做ML系统多少都会看看各种框架做的一些新特性,Torch2.0的一些东西可以关注.

- [Torch](https://github.com/pytorch/pytorch)
- [TensorFlow](https://github.com/tensorflow/tensorflow)
- [Mindspore](https://gitee.com/mindspore/mindspore)
- [Jax](https://github.com/google/jax)
- [oneflow](https://github.com/Oneflow-Inc/oneflow)
- [paddle](https://github.com/PaddlePaddle/Paddle)
- [ivy](https://github.com/unifyai/ivy)

## [](https://fazzie-key.cool/2023/02/21/MLsys/#AI-compiler "AI compiler")AI compiler

编译这块我接触的不多,这两个项目可以参考

### [](https://fazzie-key.cool/2023/02/21/MLsys/#TVM "TVM")TVM

- [github](https://github.com/apache/tvm)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#BladeDISC "BladeDISC")BladeDISC

- [github](https://github.com/alibaba/BladeDISC)

## [](https://fazzie-key.cool/2023/02/21/MLsys/#Distributed-training "Distributed training")Distributed training

对于大模型而言,分布式训练是比不缺的,除了最基本的分布式数据并行,各种混合并行策略,显存优化策略必不可缺,也因此出现了一系列的大模型训练框架.

### [](https://fazzie-key.cool/2023/02/21/MLsys/#ColossalAI "ColossalAI")ColossalAI

All in 参与的项目,刚刚star接近15k了,在torch生态下支持各种并行和显存优化策略,给自己打个广告,欢迎各位朋友star,关注

- [github](https://github.com/hpcaitech/ColossalAI)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#Megatron-LM "Megatron-LM")Megatron-LM

NVIDIA做的模型并行库,也是最早开源的模型并行,但对缺乏分布式训练背景的人使用不太友好.

- [Github](https://github.com/NVIDIA/Megatron-LM)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#Deepspeed "Deepspeed")Deepspeed

微软的大模型训练框架,核心技术是Zero infinity相关的一系列paper,使用Megatron-LM作为张量并行的支持

- [github](https://github.com/microsoft/DeepSpeed-MII)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#huggingface-accelerate "huggingface accelerate")huggingface accelerate

huggingface的加速器,对各种不同硬件做了兼容,在huggingface生态下非常好用,在分布式上做了比较多的封装,可以直接调用Deepspeed.

- [github](https://github.com/huggingface/accelerate)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#Bagua "Bagua")Bagua

Bagua在多机通讯上做了非常多的工作,对allreduce等分布式通讯做了不少优化.

- [github](https://github.com/BaguaSys/bagua)

### [](https://fazzie-key.cool/2023/02/21/MLsys/#lightning "lightning")lightning

lightning集成了各种分布式后端,可以很方便的启动各种分布式策略,lightning本身是一套更好的MLflow,设计理念是算法和工程分开,提供了大量自定义hook,对于大型AI项目而言,是个不错的选择,但是学习门槛不低,对团队人员水平要求比较高.

- [github](https://github.com/Lightning-AI/lightning)

## [](https://fazzie-key.cool/2023/02/21/MLsys/#Distributed-Communication "Distributed Communication")Distributed Communication

在nccl之前MPI是分布式通讯的主要方式,简单了解一下还是有必要的

### [](https://fazzie-key.cool/2023/02/21/MLsys/#MPI-the-Message-Passing-Interface "MPI(the Message Passing Interface)")MPI(the Message Passing Interface)

- [MPI Tutorials](https://mpitutorial.com/tutorials/)

## [](https://fazzie-key.cool/2023/02/21/MLsys/#Tips "Tips")Tips

最后简单给几个Tips,

- 尽可能参与到业界的项目去,多注重落地,在实验玩一些很fancy但不落地的东西意义不大,目前MLsys业界比学界跑的快太多了,学界比较有影响力的工作往往也是何大公司合作的,比如前面提到的[Alpa](https://arxiv.org/abs/2201.12023)的实验就是在Google的TPU集群和AWS上做的,一般的lab更本没有条件.
- 及时关注各种Blog,新的论文,这个领域每天都有新的东西出来,比如我就比较喜欢每天去twitter,一些Discord channel挖宝,总能带来一些惊喜
- Mlsys是一个交叉全面的领域,除了软件和算法,懂点基本的硬件,学会做产品也很重要,更多时候易用性大于性能.
- 新的东西肯定是看不完的,抓住主干即可,在自己做的部分保证专注
- 拥抱开源社区,不管是github还是huggingface,你永远想象不出开源社区的老哥的创造力



下面是从 0 到进入 LLM/MLSys 前沿的学习路径：
### ✅ 1. 基础阶段（打牢数学与编程基础）

- **数学：**
    - 线性代数（矩阵运算、特征值分解）
    - 概率论与统计
    - 微积分
    - 优化基础（梯度下降）
- **编程：**
    - Python 基础
    - Numpy / Matplotlib / Pandas
    - Jupyter Notebook 使用
---
### ✅ 2. 机器学习基础（模型+理论）
- 经典算法：
    - 线性回归、逻辑回归
    - 决策树、随机森林、SVM
    - KNN、KMeans、PCA
- 框架：
    - Scikit-learn
- 课程推荐：
    - [吴恩达《机器学习》](https://www.coursera.org/learn/machine-learning)
    - 周志华《机器学习》教材
---
### ✅ 3. 深度学习进阶
- 理论 + 实践：
    - 多层感知机（MLP）
    - 卷积神经网络（CNN）
    - 循环神经网络（RNN/LSTM）
    - Transformer、注意力机制
- 框架：
    - PyTorch / TensorFlow
    - HuggingFace Transformers
- 课程推荐：
    - [深度学习专项课程（吴恩达）](https://www.coursera.org/specializations/deep-learning)
    - CS231n（Stanford）
    - CS224n（Stanford NLP）
---

### ✅ 4. 大模型与自然语言处理（NLP）
- 学习内容：
    - 语言模型（BERT、GPT）
    - 预训练 + 微调
    - Prompt 工程、RAG、LoRA、量化等
- 实践：
    - 用 HuggingFace 加载/微调模型
    - 搭建 Chatbot、RAG 系统
- 推荐项目：
    - LangChain, LlamaIndex, OpenChat

---
### ✅ 5. MLSys & 工程实践

- 系统设计：
    - GPU 调度、分布式训练策略（DDP, ZeRO）
    - 自动混合精度（AMP）、模型并行、数据并行
    - 模型压缩、部署优化（ONNX, TensorRT）
- 工具栈：
    - Ray, DeepSpeed, Megatron-LM
    - Docker, Kubernetes, MLflow
    - CUDA 编程（选学）
- 课程推荐：
    - CMU 10-414 / 10-418：ML Systems
    - Stanford CS329D：Machine Learning Systems Design

MLSys 看作是“机器学习+分布式系统+系统优化”三者的交集。

## 🧱 第一阶段：基础准备

### 🧑‍💻 编程能力

- 熟练掌握：**Go、Rust、C++ 或 Java/Python**
    
- 理解多线程、多进程、协程（如 Go 的 goroutine）
    

### 📘 操作系统知识（理解底层机制）

- 进程、线程、锁、IO、多路复用、内存管理、虚拟化
    
- 推荐课程：
    
    - MIT 6.828（操作系统）
        
    - 《操作系统：三易》
        

### 🌐 网络基础

- TCP/IP、DNS、Socket 编程
    
- HTTP/2、QUIC、gRPC 等现代通信协议
    

---

## ⚙️ 第二阶段：分布式系统基础原理

### 📚 推荐教材

- 《Designing Data-Intensive Applications》（DDIA）⭐️⭐️⭐️⭐️⭐️（必须看）
    
- 《分布式系统原理与范式》（Tanenbaum）
    
- 《The Art of Multiprocessor Programming》（并发相关）
    

### 🧠 学习核心概念

- CAP 定理（Consistency, Availability, Partition tolerance）
    
- BASE 原则、最终一致性
    
- 分布式存储 vs 分布式计算 vs 分布式协调
    

---

## 🔧 第三阶段：分布式系统关键技术

### ✅ 一致性协议

- 经典算法：
    
    - Paxos、Raft（共识算法）
        
- 应用场景：
    
    - 分布式锁、协调（Zookeeper、etcd）
        

### ✅ 容错机制

- 重试、幂等、超时、心跳、选主
    
- Checkpoint、日志复制、快照恢复
    

### ✅ 分布式通信

- RPC 框架（gRPC、Thrift、Dubbo）
    
- 服务发现（Consul、etcd）
    

---

## 💾 第四阶段：分布式系统实战组件

### 🔨 实践构建以下系统的“简化版”或源码阅读：

|组件类型|工具/框架|建议实践|
|---|---|---|
|分布式 KV 存储|etcd、Consul、TiKV（Rust）|✅|
|分布式队列|Kafka、Pulsar、NATS|✅|
|分布式数据库|CockroachDB、TiDB、YugabyteDB|⭐️|
|分布式锁 / 协调|Zookeeper、etcd|✅|
|分布式计算|Spark、Ray、Dask|⭐️|
|分布式训练框架|PyTorch DDP、Horovod、DeepSpeed|⭐️⭐️|

---

## ☁️ 第五阶段：云原生与工程部署

### 📦 容器技术

- Docker、OCI、容器镜像与网络
    
- 容器调度（Kubernetes 核心原理）
    

### 📡 服务治理

- Service Mesh（Istio、Linkerd）
    
- API Gateway、负载均衡（Envoy）
    

### 📈 系统 observability

- 日志（EFK）、指标（Prometheus + Grafana）、分布式追踪（Jaeger）
    

---

## 🔬 第六阶段：进阶方向

### ☁️ 云原生分布式系统

- Kubernetes 深入
    
- 微服务架构与治理
    
- CI/CD、DevOps
    

### 🤖 MLSys / AI Infra

- 分布式训练系统设计
    
- 模型并行、数据并行
    
- FSDP、ZeRO、TensorParallel
    

### 🪄 论文精读（建议选读）

- Google Spanner、Dynamo、Chubby
    
- Paxos Made Simple、Raft
    
- GFS、MapReduce、Bigtable
    

---

## 📘 路线图总结表（精简版）

| 阶段   | 学什么                       | 推荐书/课/实践                        |
| ---- | ------------------------- | ------------------------------- |
| 基础准备 | 操作系统、网络、并发                | MIT 6.828、CS144、Go并发编程          |
| 核心原理 | CAP、Paxos、Raft、共识、一致性     | DDIA、Tanenbaum                  |
| 构建能力 | 分布式存储、RPC、协调、容错           | 构建 etcd、阅读 TiKV、Kafka 源码        |
| 工程落地 | Docker、K8s、微服务            | Kubernetes Handbook、Istio 实践    |
| 深度优化 | 分布式 AI、系统调优、弹性、Serverless | Megatron、DeepSpeed、Ray、Paper 精读 |


机器学习
- 吴恩达《2022新版机器学习》课程
- 