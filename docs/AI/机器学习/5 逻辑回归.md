---
date: 2025-06-03
tags:
  - 机器学习
---

## 分类问题

![](assets/Pasted%20image%2020250602235735.png)

![](assets/Pasted%20image%2020250602235746.png)

![](assets/Pasted%20image%2020250602235806.png)

一个分类问题：根据肿瘤的大小进行预测，可能是良性的或恶性的。

函数 $f_{w,b}(x)$ 根据肿瘤的大小计算出一个值，再与阈值进行比较。



## 假设函数

![](assets/Pasted%20image%2020250602235122.png)

![](assets/Pasted%20image%2020250602235210.png)

首先介绍sigmoid函数，又称logistic函数
$$g(z)=\frac{1}{1+e^{-z}}\quad0<g(z)<1$$

当 z 取无限大时，函数趋向于1；当取无限小时，趋向于0；该函数表示结果输出为1的概率。

令
$$\text{Z}=\overrightarrow{W}\cdot\overrightarrow{X}+b$$
则逻辑回归的模型假设函数为
$$f_{\overrightarrow{W},b}(\overrightarrow{X})
=g(\text{Z})
=g(\vec{W}\cdot\vec{X}+b)=\frac{1}{1+e^{-(\overrightarrow{W}\cdot\overrightarrow{X}+b)}}$$


## 决策边界

![](assets/Pasted%20image%2020250603001953.png)
![](assets/Pasted%20image%2020250603002003.png)
![](assets/Pasted%20image%2020250603002009.png)

sigmoid 函数的意义就是将不同的模型假设函数的值区间限制在 (0,1) 的范围内，然后取阈值进行分类。

## 代价函数 

### 将线性回归代价函数应用到逻辑回归
![](assets/Pasted%20image%2020250603002057.png)
![](assets/Pasted%20image%2020250603002108.png)


首先我们回顾一下之前学过的一个代价函数，线性回归的代价函数：
$$J(w,b)=\frac{1}{2m}\sum_{i=1}^m\bigl(f_{w,b}\bigl(x^{(i)}\bigr)-y^{(i)}\bigr)^2$$
如果把此代价函数用在逻辑回归中会怎么样？如上图。

我们发现，此函数不是凸函数，如果用梯度下降的话，找到的是局部最优，不是整体最优。

	凸函数有个很好的性质，只要能证明是凸函数，最终解一定是全局最优解，即局部最小值是全局最小值。

因此，得出的结论是，最小二乘法代价函数不适合逻辑回归。

### 逻辑回归代价函数

![](assets/Pasted%20image%2020250603002600.png)![](assets/Pasted%20image%2020250603002614.png)![](assets/Pasted%20image%2020250603002621.png)
我们定义**逻辑回归的损失函数**如下（后续说明为何定义该函数）：
$$L\left(f_{\overrightarrow{{{w}}},b}\left(\overrightarrow{{{x}}}^{(i)}\right),y^{(i)}\right)=\left\{\begin{array}{rl}{{-\log\left(f_{\overrightarrow{{{w}}},b}\left(\overrightarrow{{{x}}}^{(i)}\right)\right)}}&{{\mathrm{if~}y^{(i)}=1}}\\ {{-\log\left(1-f_{\overrightarrow{{{w}}},b}\left(\overrightarrow{{{x}}}^{(i)}\right)\right)}}&{{\mathrm{if~}y^{(i)}=0}}\end{array}\right.$$

因此，逻辑回归的代价函数如下：

$$J(\overrightarrow{W},b)=\frac{1}{m}\sum_{i=1}^{m}L\bigl(f_{\overrightarrow{W},b}\bigl(\overrightarrow{x}^{(i)}\bigr),y^{(i)}\bigr)$$

![](assets/Pasted%20image%2020250603140322.png)

由于y的取值只能是0或1，所以**简化**后的损失函数及代价函数如下：
$$\begin{aligned}
&L{\big(}f_{\overrightarrow{{{w}}},b}{\big(}\overrightarrow{{{x}}}^{(i)}{\big)},y^{(i)}{\big)}=-y^{(i)}\mathrm{log}{\Big(}f_{\overrightarrow{{{w}}},b}{\big(}\overrightarrow{{{x}}}^{(i)}{\big)}{\Big)}-{\big(}1-y^{(i)}\big){\mathrm{log}}{\Big(}1-f_{\overrightarrow{{{w}}},b}{\big(}\overrightarrow{{{x}}}^{(i)}{\big)}{\Big)} \\
&J(\overrightarrow{w},b)=\frac{1}{m}\sum_{i=1}^{m}[L\big(f_{\overrightarrow{w},b}\big(\overrightarrow{x}^{(i)}\big),y^{(i)}\big)]
\end{aligned}$$

**为什么选择这个函数作为代价函数呢？**

这个特定的代价函数是使用称为最大似然估计的统计原理从统计中推导出来的。最大似然估计是一种常用的参数估计方法，它基于样本数据，通过寻找最有可能产生这些数据的参数值，来确定模型的参数。

## 梯度下降

![](assets/Pasted%20image%2020250603140955.png)
![](assets/Pasted%20image%2020250603141002.png)


$$\begin{aligned}
w_{j}&=w_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}\big(x^{(i)}\big)-y^{(i)})x_{j}^{(i)}\\
b &=b-\alpha\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})
\end{aligned}$$

注意：形式与线性回归一样，但是$f_{w,b}(x)$ 表达式不同。

