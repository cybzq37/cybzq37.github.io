## 特征缩放

![](assets/Pasted%20image%2020250602222955.png)


![](assets/Pasted%20image%2020250602223025.png)

![](assets/Pasted%20image%2020250602230213.png)

房价

$$
pr i c e=w_{1}x_{1}+w_{2}x_{2}+b
$$
当特征的可能值较大时，如房间大小($x_{1}$)，则其参数($w_{1}$)的合理值将相对较小
当特征的可能值较小时，如卧室数量($x_{2}$)，则其参数($w_{2}$)的合理值将相对较大

特征散点图如左上时，其代价函数在等高线图中的样子如右上所示，为椭圆形，梯度下降可能会在它最终找到全局最小值之前来回弹跳很长的时间。

所以当你有不同的特征，它们的取值范围非常不同时，它可能会导致梯度下降运行缓慢，但重新缩放不同的特征，使它们都具有可比较的取值范围，让梯度下降法运行得更快。

**特征缩放（Feature Scaling）** 通过使每个输入值在大致相同的范围内来加速梯度下降。

这是因为梯度在小范围内下降快，而在大范围内下降较慢；另外，对于不平整的变量，梯度在下降至最优值的过程中会出现降低效率的震荡。

## 归一化

![](assets/Pasted%20image%2020250602230221.png)
![](assets/Pasted%20image%2020250602230231.png)


将 **特征缩放（feature scaling）** 和 **均值归一化（mean normalization）** 两种技术结合使用。  

**特征缩放** 将输入值除以输入变量的范围（即最大值减去最小值），从而得到一个大小为 1 的新范围。  

**均值归一化** 涉及从输入变量的值减去输入变量的平均值，从而导致输入变量的新平均值为 0。  

**平均值归一化方法（Mean normalization）**

如果要将该值映射到\[-1,1\]区间内，则需要计算特征的平均值$X_{Mean}$，使用平均值归一化方法公式为


$$x=\frac{X-X_{Mean}}{X_{Max}-X_{Min}}$$

**Z-score标准化方法（Z-score Normalization）**

标准化法需要提前计算特征的均值$X_{Mean}$和标准差$\sigma$，标准化后的变量值围绕0上下波动，大于0说明高于平均水平，小于0说明低于平均水平。
$$x={\frac{X-X_{M e a n}}{\sigma}}$$


## 特征缩放的范围

![](assets/Pasted%20image%2020250602231227.png)

最好是-1到1，实在不行，最大是-3到3，最小是-0.3到0.3，都能接受。

基本意思是，保证各个特征的数量级一致，通过缩放尽量让所有特征的取值在差不多范围，在一个数量级以内的特征可以不考虑缩放，这样它们的变化对预测值的影响都是接近的。

## 检查梯度下降是否收敛

在运行梯度下降时，怎样才能知道梯度下降是否有效，是否正在找到全局最小值或接近它的值？为了能够选择更好的学习率。

判断梯度下降运行是否良好，用**学习曲线**（learning curve），横轴为梯度下降迭代次数，纵轴为代价函数J。

![](assets/Pasted%20image%2020250602231402.png)

当运行良好时，学习曲线是一直下降的，直到变平（即收敛）；当出现先下降后又上升的情况时，可能是学习率错误，或者程序错误。查看此学习曲线，您可以尝试发现梯度下降是否收敛。

另一种方法是**自动收敛测试**（Automatic convergence test）
令$\epsilon$ ”epsilon“等于一个非常小的数，如果代价J在一次迭代中减少的幅度小于这个数字epsilon,那么很可能位于学习曲线的平坦部分，可以宣布收敛。但是选择正确的阈值epsilon非常困难。


## 学习率
![](assets/Pasted%20image%2020250602231533.png)

![](assets/Pasted%20image%2020250602231601.png)
当学习率 $\alpha$ 太小，则梯度下降太慢；而当学习率 $\alpha$ 太大，则梯度可能不会下降，也因此不会收敛。因此有时需要观察并进行调整。

- 学习率过大，调小学习率
- 程序有错误，查看公式是否写错，比如把减号写成加号

**尝试不同的学习率并画对应的学习曲线**

……0.001，0.003，0.01，0.03，0.1，0.3，1 …… 

## 特征工程

![](assets/Pasted%20image%2020250602231906.png)

![](assets/Pasted%20image%2020250602231914.png)

![](assets/Pasted%20image%2020250602231922.png)

利用领域知识和现有数据，创造出新的特征，用于机器学习算法；可以手动（manual）或自动（automated）。

如果新特征是原始特征的次方，那么特征缩放会非常重要





























## 参数计算

### 正规方程

梯度下降是最小化 $J$ 的一种方法。第二种方法是**正规方程（Normal Equation）**，它显式地执行最小化，而不使用迭代式的算法。

$$\theta = (X^TX)^{-1}X^Ty$$

正规方程方法中，无需做特征缩放。两种方法的对比如下：

|    梯度下降     |            正规方程             |
| :---------: | :-------------------------: |
|   需要选择学习率   |          不需要选择学习率           |
|   需要多次迭代    |            不需要迭代            |
|  $O(kn^2)$  | $O(n^3)$，需要计算 $(X^TX)^{-1}$ |
| 当 n 较大时效果很好 |         当 n 较大时速度较慢         |

不过正规方程方法要求 $X^TX$ 可逆。$X^TX$ 不可逆的原因有两种可能：

1. 列向量线性相关：即训练集中存在冗余特征（特征线性依赖），此时应该剔除掉多余特征；
2. 特征过多（多于样本数量）：此时应该去掉影响较小的特征，或引入正则化（regularization）项。

### 正规方程的推导过程

把数据集表示为矩阵

$$X = \left( \begin{matrix} x\_{11} & x\_{12} & \cdots & x\_{1d} & 1 \\\ x\_{21} & x\_{22} & \cdots & x\_{2d} & 1 \\\ \vdots & \vdots & \ddots & \vdots & \vdots \\\ x\_{m1} & x\_{m2} & \cdots & x\_{md} & 1 \\\ \end{matrix} \right) = \left( \begin{matrix} x\_{1}^T & 1 \\\ x\_{2}^T & 1 \\\ \vdots & \vdots \\\ x\_{m}^T & 1 \\\ \end{matrix} \right)$$

同时将标签也写成向量形式

$$y = (y\_1;y\_2;...;y\_m)$$

由均方误差最小化，可得

$$\theta^* = arg\_{\theta}min(y-X\theta)^T(y-X\theta)$$

其中，$\theta^*$表示 $\theta$ 的解。

令

$$E\_{\theta} = (y-X\theta)^T(y-X\theta)$$

对 $\theta$ 求导得到

$$
\begin{equation}
\begin{split}
\frac{\partial E\_{\theta}}{\partial \theta}&=-X^T(y-X\theta) + (y^T - \theta^TX^T) \cdot (-X)\\\
&=2X^T(X\theta - y)
\end{split}
\end{equation}
$$

令上式为 0，有

$$2X^T(X\theta - y) = 0$$

$$X^TX\theta = X^Ty$$

最终得到

$$\theta = (X^TX)^{-1}X^Ty$$

当 $X^TX$ 不为满秩矩阵（不可逆）时，可解出多个 $\theta$ 使均方误差最小化。因此将由学习算法的归纳偏好来决定选择哪一个解作为输出，常见的做法就是引入正则化项。