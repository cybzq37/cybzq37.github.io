# 过拟合

## 欠拟合、过拟合

![](assets/Snipaste_2023-06-13_16-57-47.png)

左图，欠拟合（underfit），也称作高偏差（High-bias），即对于当前数据集的拟合程度不够，欠拟合的特征是在训练集和测试集上的准确率都不好；

中图，拟合刚好的状态，具有泛化能力；

右图，过拟合（overfit），也称作高方差（High variance），过拟合对于当前训练数据拟合得太好了，以至于模型只在当前的训练集上表现很好，而在其他数据集上表现就不是那么好，所以过拟合的特征是在训练集上准确率很高而在测试集上表现一般。

## 解决过拟合

方法：
	1. 收集更多的数据
	2. 减少特征数量
	3. 正则化


### 特征工程

### 正则化

将正则化应用在代价函数中，使用正则化来减小参数的大小

![](assets/Snipaste_2023-06-14_09-11-09.png)

如何理解上式？

因为$w_3^2$和$w_4^2$前的系数非常大，要想使整个式子最小化，只能令$w_3^2$和$w_4^2$ 非常小（$\approx0$），$w_3$和$w_4$ 即为惩罚项。
对于右图过拟合状态，此时$w_3$和$w_4$ 权重衰减，这样就会降低高阶项对整个函数的影响，使得拟合的函数变得比较平滑。

如果有很多的特征，那如何选择惩罚项呢？

如果有非常多的特征，你可能不知道那些特征重要，以及需要惩罚的特征。通常实现正则化的方式是惩罚**所有**的特征。（==为什么惩罚有利特征==）

对于预测房价实例，比如有100个特征，正则化后的代价函数如下：
$$J(\vec{w},b)=\frac{1}{2m}\sum_{i=1}^{m}(f_{\vec{w},b}\big(\vec{x}^{(i)}\big)-y^{(i)}\big)^2+\frac{\lambda}{2m}\sum_{j=1}^{n}\omega_{j}^2$$
把上式分为两部分，左边部分即为原始的代价函数，右边部分为正则化项。λ为超参数，通常会取一个较大的数。

为了最小化整个代价函数，当λ是固定的，那么就要减小$w_1$到$w_n$的值。加入正则项后，$w_1$到$w_n$均会减小，也就是使得权重衰减，这样就会降低高阶项对于整个函数的影响，使得估计函数变得比较平滑。

我们还将λ除以2m,这样这里的第一项和第二项都在2m上按比例缩放，会更改容易选择λ的值。按照惯例我们不会因为参数b太大而惩罚它，在实践中，做与不做几乎没有什么区别。

因此，总结一下这个修改后的代价函数：
![](assets/Snipaste_2023-06-14_09-37-48.png)

我们想要最小化\[原始代价函数即均方误差项+第二项即正则化项\]

λ : 可以控制两个不同目标之间的取舍。

此函数有两个目的，目的一：最小化预测值与真实值之间的误差，更好的拟合训练集。目的二：试图减小$w_j$ ，使假设函数变得“简单”，防止过度拟合。

**两者相互平衡，从而达到一种相互制约的关系，最终找到一个平衡点，从而更好地拟合训练集并且具有良好的泛化能力。**


不同的λ值有什么影响？

使用线性回归的房价预测示例。

如果，λ等于0，那么正则项等于零，即根本没有使用正则化，会过度拟合。
![](assets/Snipaste_2023-06-14_09-59-48.png)

当λ非常非常大时，例如$\lambda=10^{10}$,那么$w_1到w_4$几乎等于0，只剩常数b项，此时会欠拟合。

![](assets/Snipaste_2023-06-14_10-00-42.png)

线性回归的正则化方法：
![](assets/Snipaste_2023-06-14_10-13-49.png)

$$\begin{aligned}
& w_{j}=w_{j}-\alpha\left[\frac{1}{m}\sum_{i=1}^{m}\left[(f_{\vec{{{w}}},b}\big(\vec{x}^{(i)}\big)-y^{(i)}\big)x_{j}^{(i)}\right]+\frac{\lambda}{m}w_{j}\right]  \\
&b=b-\alpha\frac{1}{m}\sum_{i=1}^m(f_{\vec{\mathbf{w}},b}(\vec{\mathbf{x}}^{(i)})-y^{(i)})
\end{aligned}$$

逻辑回归的正则化方法：
![](assets/Snipaste_2023-06-14_10-18-04.png)

参考资料：[吴恩达机器学习笔记（三）正则化](https://zhuanlan.zhihu.com/p/75364861)

